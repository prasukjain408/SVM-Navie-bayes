{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "  - What is Information Gain?\n",
        "\n",
        "A measure of impurity reduction: Information Gain quantifies the reduction in entropy (a measure of impurity or uncertainty) after a dataset is split based on a particular feature.\n",
        "\n",
        "Decision-making criteria: It is used in algorithms like ID3 to decide which feature to use to split the data at each node of the tree.\n",
        "\n",
        "Higher is better: A higher Information Gain means the feature is more useful for splitting the data, as it creates more distinct and predictable groups.\n",
        "\n",
        "  - How is it used in decision trees?\n",
        "\n",
        "Calculate entropy: First, the entropy of the dataset before any split is calculated. Entropy measures the level of randomness or impurity in the data.\n",
        "\n",
        "Calculate potential gains: For each potential feature to be split on, calculate the weighted average of the entropy of the resulting subsets.\n",
        "\n",
        "Find the feature with the highest gain: The feature that results in the largest reduction in entropy (i.e., the highest Information Gain) is chosen as the split point for the current node.\n",
        "\n",
        "Repeat: This process is repeated recursively for each new child node with the subset of data it receives, until a stopping criterion is met (e.g., the nodes are \"pure\" or a maximum depth is reached).\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "  - Strengths and weaknesses\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Strength: Highly efficient for large datasets due to its simpler, faster calculations.\n",
        "\n",
        "Strength: Can effectively handle imbalanced datasets by favoring balanced splits, which can help address class imbalance problems.\n",
        "\n",
        "Weakness: Its speed can sometimes come at the cost of less-nuanced splitting decisions compared to entropy.\n",
        "\n",
        "Entropy\n",
        "\n",
        "Strength: Can lead to more accurate models on certain datasets by making finer, more precise splits.\n",
        "\n",
        "Strength: Provides a richer, more nuanced measure of information and uncertainty, which can be valuable for analysis.\n",
        "\n",
        "Weakness: The higher computational cost makes it less ideal for extremely large datasets or applications where speed is a top priority.\n",
        "\n",
        "Use cases and practical application\n",
        "\n",
        "Use Gini Impurity when speed is critical and training time needs to be minimized. This is especially relevant for large datasets where the computational difference becomes more pronounced.\n",
        "\n",
        "Use Entropy when you need a more detailed analysis of the information gain or when a small improvement in model accuracy is prioritized over training speed, particularly with datasets that have more evenly distributed classes.\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "  - Pre-pruning, or early stopping, is a technique in decision trees that halts the growth of a tree during its construction to prevent overfitting. It works by setting criteria, such as a maximum depth, a minimum number of samples per leaf, or a minimum information gain for a split, that the tree must meet before it continues to split nodes.\n",
        "  \n",
        "Purpose: To prevent the tree from becoming too complex and memorizing the training data, which leads to poor performance on new data.\n",
        "\n",
        "Method: Stops the tree-building process before the tree reaches its full, potentially over-complex size.\n",
        "\n",
        "Criteria: Common conditions used to stop the growth include:\n",
        "\n",
        "Maximum depth: Stops the tree from growing beyond a specified number of levels.\n",
        "\n",
        "Minimum samples per leaf: Stops splitting a node if the number of samples in it falls below a set threshold.\n",
        "\n",
        "Minimum samples per split: Requires a minimum number of samples to be present in a node before it can be split.\n",
        "\n",
        "Minimum impurity decrease: Halts the tree from growing if a split does not significantly improve purity (e.g., using information gain).\n",
        "\n",
        "Advantage: It is faster than post-pruning because it avoids the computational cost of building a full tree first.\n",
        "\n",
        "Disadvantage: There is a risk of underfitting if the stopping criteria are too strict, causing the model to stop growing prematurely and fail to capture important patterns.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H9Xib_YZfAvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "# Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset as an example\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini impurity as the criterion\n",
        "# The 'criterion' parameter is set to 'gini' for Gini Impurity\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "\n",
        "# Create a Pandas Series for better visualization of feature importances\n",
        "importance_df = pd.Series(feature_importances, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances (Gini Impurity):\")\n",
        "print(importance_df)\n",
        "\n",
        "# Optional: Evaluate the model (accuracy)\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy on Test Set: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6P9u7gghpgX",
        "outputId": "e19657ba-f850-49d6-f132-0b66c55d1290"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Gini Impurity):\n",
            "petal length (cm)    0.893264\n",
            "petal width (cm)     0.087626\n",
            "sepal width (cm)     0.019110\n",
            "sepal length (cm)    0.000000\n",
            "dtype: float64\n",
            "\n",
            "Model Accuracy on Test Set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "  - A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its primary goal is to find the optimal boundary, or hyperplane, that separates data points of different classes in a high-dimensional space. A key principle of SVMs is to maximize the margin, the distance between the hyperplane and the nearest data points, known as support vectors.\n",
        "\n",
        "Key concepts of SVMs\n",
        "\n",
        "Hyperplane: The decision boundary that separates different classes of data. In a 2D space, this boundary is a line, and in higher dimensions, it is a plane or a hyperplane.\n",
        "\n",
        "Support vectors: The data points from each class closest to the hyperplane, which determine its position and orientation.\n",
        "\n",
        "Margin: The distance between the hyperplane and the support vectors. Maximizing this margin helps improve generalization and reduce overfitting.\n",
        "\n",
        "Kernel trick: A technique to handle non-linearly separable data by implicitly mapping it to a higher-dimensional space where a linear separation is possible. Common kernels include Radial Basis Function (RBF) and polynomial kernels.\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "  \n",
        "  - The Kernel Trick is a technique used in Support Vector Machines (SVMs) to classify non-linear data by implicitly mapping it to a higher-dimensional space, allowing a linear classifier to find a separating hyperplane. Instead of explicitly performing this computationally expensive transformation, the kernel trick uses a kernel function to calculate the dot product of the transformed data points directly from the original data. This saves computational cost and time by avoiding the need to work in a high-dimensional space.\n",
        "\n",
        "How it works\n",
        "\n",
        "Problem: The original data is not linearly separable in its current space, meaning a straight line cannot divide the classes.\n",
        "\n",
        "Solution: To make the data separable, an SVM algorithm can map the data to a higher-dimensional space using a feature map, \\(\\phi (x)\\). In this new space, the data might be linearly separable, and a linear classifier can be used.\n",
        "\n",
        "The trick: Calculating the coordinates in this new, higher-dimensional space can be computationally very expensive. The kernel trick provides a shortcut by using a kernel function, \\(K(x,y)\\), which directly computes the dot product of the transformed points, \\(\\phi (x)\\cdot \\phi (y)\\), without ever having to compute the transformed vectors themselves.\n",
        "\n",
        "Benefit: The SVM algorithm then operates on these dot products, which are equivalent to the dot products in the higher-dimensional space, allowing it to efficiently find a linear decision boundary that separates the data in the new feature space."
      ],
      "metadata": {
        "id": "Z6bbGRY7iD6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "# kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the linear kernel SVM\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train an SVM classifier with an RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the RBF kernel SVM\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"The Linear Kernel SVM performed better.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"The RBF Kernel SVM performed better.\")\n",
        "else:\n",
        "    print(\"Both SVMs performed equally well.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG5LgicTjBlS",
        "outputId": "679acbd4-ac9c-463e-ac98-ef3f6d38ce80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 1.0000\n",
            "Accuracy of SVM with RBF Kernel: 0.8056\n",
            "The Linear Kernel SVM performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "  - The Naïve Bayes classifier is a supervised machine learning algorithm based on Bayes' theorem that uses conditional probability to predict the class of a data point. It is called \"naïve\" because it makes the simplifying and often unrealistic assumption that all the features used for classification are independent of one another. This assumption allows the algorithm to make calculations much simpler and faster, even if it means a loss of accuracy in some real-world scenarios where features are not truly independent.\n",
        "\n",
        "Why it's called \"naïve\"\n",
        "\n",
        "Assumption of independence: The \"naïve\" part of the name refers to its core, simplistic assumption that all features are independent of each other.\n",
        "\n",
        "Real-world vs. assumption: In practice, features are often not independent. For example, in text classification, the presence of one word can be dependent on the presence of another.\n",
        "\n",
        "Simplifies calculation: The naïve assumption makes the complex calculations much more computationally efficient, even though it might not perfectly represent the real-world data.\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "  - Gaussian Naïve Bayes:\n",
        "\n",
        "Assumes features follow a normal distribution (bell curve)\n",
        "\n",
        "Used for continuous data like age, weight, or temperature\n",
        "\n",
        "Calculates probabilities using the Gaussian probability density function\n",
        "\n",
        "Multinomial Naïve Bayes:\n",
        "\n",
        "Assumes features are discrete counts\n",
        "\n",
        "Often used in text classification where features represent the number of times a word appears in a document\n",
        "\n",
        "Bernoulli Naïve Bayes:\n",
        "\n",
        "Assumes features are binary (either present or absent)\n",
        "\n",
        "Used for tasks like spam detection or classifying emails as spam/not spam\n",
        "\n",
        "Important points to remember:\n",
        "\n",
        "All three Naïve Bayes algorithms share the \"naive\" assumption that features are independent given the class label. This means the presence or value of one feature doesn't influence the probability of any other feature.\n",
        "Choosing the right Naïve Bayes variant depends on the nature of your data. If your features are continuous, use Gaussian Naïve Bayes. If your features are counts, use Multinomial Naïve Bayes. If your features are binary, use Bernoulli Naïve Bayes.   -\n"
      ],
      "metadata": {
        "id": "bMZrl1PFjpVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Breast Cancer Dataset\n",
        "# Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "# dataset and evaluate accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "# We'll use 80% of the data for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# 4. Train the classifier on the training data\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred, target_names=breast_cancer.target_names)\n",
        "\n",
        "# 7. Print the results\n",
        "print(\"Gaussian Naïve Bayes Classifier on Breast Cancer Dataset\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE84bk7MlEKB",
        "outputId": "bef5d070-fea5-4a58-d86d-3d249738260e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Classifier on Breast Cancer Dataset\n",
            "------------------------------------------------------------\n",
            "Accuracy: 0.9737\n",
            "\n",
            "Confusion Matrix:\n",
            "[[40  3]\n",
            " [ 0 71]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9taubTClTzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHjZOcwQjaVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyKYzXlXh2jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwMpgPm1e5h-"
      },
      "outputs": [],
      "source": []
    }
  ]
}